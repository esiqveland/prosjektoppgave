\clearpage
\section{Software}
\label{sec:software}
For this project, we needed quite a bit a software to be sure we could generate somewhat realistic workloads.
We also thought it important with predictability and put a lot of effort towards having a controlled environment. This goes for both the software and hardware involved.

Having done a bit of performance analysis before, we are aware of how difficult it can be to get meaningful and consistent results when analyzing programs and hardware interactions. We therefore decided to base ourselves on using some code we had written earlier in a information retrieval course. It is a simple search engine and indexer written in python, that does full text searches on a indexed corpus.

\subsection{Alternatives}
There are a few open source projects already that are focused on information retrieval. We will here discuss some of the software we were considering using for this project.

The most obvious software alternative might be Lucene, but it is a heavier search engine that also requires a JVM. Additionally it is not a project and codebase we are familiar with.
This fact alone would make it hard for us to modify Lucene to our needs and be sure we got it right, and keep the level of control we wish for. The limited amount of less than 500MB of RAM was also a concern of ours when we were discussing Lucene.

Another project is the Sphinx search engine\cite{sphinx}, but this also seems like overkill for our intents and purposes.

\subsection{Setup}
As the authors had most experience with Arch Linux and it's environment, we decided to go with the distribution provided by the Arch Linux for ARM project.
The OS is distributed as a ISO image, which is written to the SD card using the UNIX disk utility {\tt dd}:
\begin{lstlisting}
dd if=inputimage.iso of=/dev/outdisk bs=1M
\end{lstlisting}

\subsection{Doing a search}
Since a significant part of our project is related to searching, we find it reasonable to include a section on how a search engine works in general.

Our engine consist of two parts: indexing and searching.
The search engine executes and scores a set of documents for given queries. For this work, it uses the dictionary, and a postings file.
The other part is the indexer, which builds an inverted index, the dictionary, and the postings file\cite{IntroIR} over a collection of documents (corpus).

When performing the indexing task, a loop runs over every document in the corpus.
For every word encountered in a document, the indexer emits a term, which is made into a token.
A token what is left after post-processing a term. This processing usually includes stemming and removal of special characters.
The token is added to a dictionary and the document id is appended to the token's postings list. A counter is also kept along with the id to keep track of multiple occurrences of the same token in the same document.
The result of this loop is a dictionary of all unique tokens encountered, along with a postings list, naming all the documents that contain each token.

The dictionary contains the token, how many times it occurs in the collection, and a pointer to where this term is stored in the postings file.
The entries can be viewed as a tuple of the format:
$$<token, count, pointer>$$
The postings pointer is in our case an integer. This integer is the byte offset in the posting file where this terms posting list starts.

The postings entries are a list of tuples, in the format $$<docID, count>$$, giving the document id where the token occurred and the number of times it was seen in that document.

\begin{figure}[h]
    \center
    \includegraphics[width=0.5\textwidth]{software/index_postings_lists}
    \caption{A sample index and postings list. Figure taken from Introduction to IR\cite{IntroIR}.}
    \label{fig:index_postings_lists_sw}
\end{figure}

\subsection{Document scoring}
In the searching step one is usually working with a query containing one or more terms. For each token the corresponding postings list is fetched from disk (and cached). Each document is then scored according to a scoring scheme and a list of the documents sorted by the score is returned to the application.

To provide a rudimentary form of scoring, we have implemented the TF-IDF scoring system for queries. We are using the weighting scheme suggested by Introduction to Information Retrieval\cite{IntroIR}, {\tt lnc.ltc} in SMART notation\cite{tfidfsmart}.


\subsection{Operating system and environment}
Our operating system of choice is the Arch Linux for ARM project\cite{archarm}. This distribution provides us with a rolling release scheme, and eases development through the big and developer friendly package system AUR. If something is missing, it can be a lot easier to add it to all of the systems through the package manager.

The systems were mostly running on Linux kernel 3.6.11-18-ARCH, compiled for armv6l. This is as reported by running {\tt uname -a} on the target platform.

The systems also had the raspberrypi-firmware-tools packages, dated 20131021-1. Compilation was done with gcc:
\begin{lstlisting}[captionpos=b,caption={Output of {\tt gcc -v} and {\tt python --version}}]
$ gcc -v
Using built-in specs.
COLLECT_GCC=gcc
COLLECT_LTO_WRAPPER=/usr/lib/gcc/armv6l-unknown-linux-gnueabihf/4.7.2/lto-wrapper
Target: armv6l-unknown-linux-gnueabihf
Configured with: /build/src/gcc-4.7.2/configure --prefix=/usr --libdir=/usr/lib --libexecdir=/usr/lib --mandir=/usr/share/man --infodir=/usr/share/info --with-bugurl=https://bugs.archlinux.org/ --enable-languages=c,c++,fortran,lto,objc,obj-c++ --enable-shared --enable-threads=posix --with-system-zlib --enable-__cxa_atexit --disable-libunwind-exceptions --enable-clocale=gnu --disable-libstdcxx-pch --enable-libstdcxx-time --enable-gnu-unique-object --enable-linker-build-id --with-ppl --enable-cloog-backend=isl --disable-ppl-version-check --disable-cloog-version-check --enable-lto --enable-gold --enable-ld=default --enable-plugin --with-plugin-ld=ld.gold --with-linker-hash-style=gnu --disable-multilib --disable-libssp --disable-build-with-cxx --disable-build-poststage1-with-cxx --enable-checking=release --host=armv6l-unknown-linux-gnueabihf --build=armv6l-unknown-linux-gnueabihf --with-arch=armv6 --with-float=hard --with-fpu=vfp
Thread model: posix
gcc version 4.7.2 (GCC)

$ python --version
Python 2.7.5
\end{lstlisting}

\subsection{Booting and deployment}
To collaborate and distribute our code, we use a git repository stored at GitHub. We also agreed upon a set folder structure to make it easier to maintain and deploy our code, storing the git directory under {\tt /home/<user>/src/project}.
The username is the same for all nodes.

We created a systemd script that runs at boot, which does a repository pull and builds all the code, then run it. So to deploy a new update, we would cut the power to the rig and start it again.
This proved very efficient in practice and saved us a lot of work manually, or by scripts, logging in and updating and relaunching each system. The boot script is given in listing \ref{lst:bootscript}.
\begin{lstlisting}[captionpos=b,caption={Our Systemd boot script. It launches a script that makes sure datetime is set before launching the update script.},label={lst:bootscript}]
#-/etc/systemd/system/pinode.service
[Unit]
Description=PI search service
ConditionPathExists=/etc/rc.local

[Service]
Type=forking
ExecStart=/etc/rc.local
TimeoutSec=0
StandardOutput=tty
RemainAfterExit=yes
SysVStartPriority=99

[Install]
WantedBy=multi-user.target

#-/etc/rc.local
su king -c "/home/king/src/project/boot_pinode.py"

#-boot_pinode.py
projectdir = "/home/king/src/project"
hostname = socket.gethostname()

os.chdir(projectdir)

def update_git():
    subprocess.call(["git", "pull"])
    subprocess.call(["make"])

def launch_load_distr():
    subprocess.call(["bin/load_distributor_bin"])

def launch_search_engine():
    subprocess.call(["bin/search_bin"])

while True:
    update_git()

    if hostname[len(hostname)-1] == '0':
        launch_load_distr()
    else:
        launch_search_engine()

\end{lstlisting}

\subsection{Load generator}
The purpose of the load generator is to generate pseudo-random queries and send these to the load distributor to simulate load on the cluster. These queries are of varying length and contains a subset of the tokens found in the dictionary. It also includes some tokens not found in the dictionary.

The load generator runs on several threads, where one thread is responsible for receiving answers for the queries, and one or more threads are responsible for generating load for the system. The load generator is implemented in python and meant to be run on a node outside the cluster.

The load generator includes some configurable parameters, most importantly the frequency of the queries being sent and the number of threads sending queries simultaneously.

\subsection{Load distribution}
One of the nodes in the cluster will be in charge of load distribution. The load distributor node will receive all the queries and then forward them to the worker nodes.
The node employs the very simple load balancing scheme of $$rand() \bmod N,$$ where N is the number of nodes in the cluster.
The messages from the load distributor to the worker nodes includes information of where to deliver the search results when they are ready.
This scheme is simple, but could also prove to be a bottleneck of the system. Seeing as all communication has to go through this node we could end up with the performance of the cluster being limited by the load distributor.

\subsection{Original python version}
The python version we were basing the program on relied heavily upon the python library Natural Language Toolkit\cite{nltk} (NLTK).
NLTK provides a lot of useful methods for processing natural languages in Python, including different tools for stemming and other tokenization tricks.
Stemming is the process of reducing inflected or conjugated words into their basic or root form. Examples include removal of prefixes, suffixes, tense and vowel changes. I.e. words like ``fishing'', ``fisher'' and ``fished'' all end up mapping to the same stem ``fish''.

The toolkit also provides stop word lists and improved splicing of natural language sentences. As this is a lot of work to implement, we have chosen to do indexing in python and deliver and calculate search results based only on the stemmed forms for the C program.

\subsection{Optimized C-port}
In practice we experienced serious performance issues with our python code, often showing response times in the hundreds of milliseconds. We therefore decided to rewrite the program using C. This proved it both challenging and useful to control what exactly is present in memory at every moment, in addition to the extra work of having to control memory allocation.

The program reads the dictionary structure into memory on startup. It provides O(1) lookup on query terms in a hash map, using the library {\tt uthash}\cite{uthash}.

\begin{lstlisting}[style=customc,captionpos=b,caption={Structure of a dictionary entry (term)},numbers=left]
typedef struct {
    char* term;
    uint32_t byte_offset;
    uint32_t occurences;
    postings_entry* posting;
    UT_hash_handle hhd;         /* makes this structure hashable */
} dictionary_entry;
\end{lstlisting}
\subsubsection{uthash}
{\tt uthash} is a small library with a set of convenient preprocessor macros for creating and managing hash tables in C.
{\tt uthash} does constant time $$O(1)$$ lookups, inserts and deletes, and comes with a number of different hash functions in case your key domain is not well suited for the default hash function.
{\tt uthash} can hash strings, integers, pointers and the bytes in a structure a pointer points to.

\subsubsection{Experiences}
The first testings of the C-port showed significant improvements in delivery speed.
It further confirmed that managing memory yourself is desirable for performance, and is especially important in an environment where resources are scarce.
When blasting the searching code with queries, we quickly maxed out the available memory.
These memory leaks were later fixed, and the program now experiences constant memory usage.

The difference in developing platform, with x86\_64 Darwin compared to the ARMv6 runtime environment did not impose any extra difficulties, except for a single case of some odd pointer behavior.
The rest of the time, the code behaved as expected from one platform to the other, for both Python and C.

For comparison between the two languages, we created a test set of 100,000 queries. We then ran this test set against the reference MacBook computer, and on the pi, both for Python and {C}.
The results are given in Table \ref{tbl:runtimes_ports}.

For the relatively quick {Core i5}, the python code need 10 times longer to run the queries than the C code, and seem reasonable compared to known sources.
The Python code is however detrimental to the performance for the weaker PI. The same 100,000 queries takes 84 seconds to run in C, but takes 24 times longer in python.

\begin{table}[h]
	\begin{center}
	\begin{tabular}{|r|r|r|}
	\hline
	   & \multicolumn{1}{|c|}{PI}     & \multicolumn{1}{|c|}{MacBook} \\
	\hline
	C      & 84.748   & 4.782     \\
	\hline
	Python & 2046.768 & 49.411    \\
	\hline
	\end{tabular}
	\caption{Runtime (in seconds) for C and Python code on 100,000 queries.}
	\label{tbl:runtimes_ports}
	\end{center}
\end{table}
